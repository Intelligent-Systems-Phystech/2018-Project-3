\documentclass[12pt,twoside]{article}
    \usepackage{jmlda}
    \usepackage{booktabs}
    %\NOREVIEWERNOTES
    \title
        {Динамическое выравнивание многомерных временных рядов}
    \author
        {Гончаров~А.\,В., Моргачев~Г.\,И., Смирнов~В.\,, Липницкая~Т.\,} % основной список авторов, выводимый в оглавление
    \thanks{
        Работа выполнена при финансовой поддержке РФФИ, проект \No\,00-00-00000.
        Научный руководитель:  Гончаров~А.\,В.
        Задачу поставил:  Гончаров~А.\,В.
        Консультант:  Гончаров~А.\,В.
    }
    \email
        {morgachev.gi@phystech.edu, smirnov.vs@phystech.edu, tanya.lipnizky@yandex.ru}
    \organization
        {МФТИ}

    \abstract{
        В данной работе исследуется кластеризация многомерных временных рядов с использованием алгоритма DTW. При использовании DTW в многомерном случае возникает проблема определения функций расстояния между элементами временных рядов. Основной целью статьи является нахождение зависимости качества кластеризации от выбора этой функции расстояния. В связи с повышением размерности возникает вопрос эффективности и применимости DTW на многомерных рядах. В качестве прикладной задачи исследуется кластеризация размеченных данных о деятельности человека полученных с акселерометра. Оценка качества кластеризации производится при сравнении с результатами кластеризации на основе авторегрессионной модели и анализу распределения классов данных в полученных кластерах.

        \bigskip
        \textbf{Ключевые слова}: \emph {временные ряды, многомерные временные ряды, DTW}.
    }
    
    % \titleEng
    %     {JMLDA paper example: file jmlda-example.tex}
    % \authorEng
    %     {Author~F.\,S.$^1$, CoAuthor~F.\,S.$^2$, Name~F.\,S.$^2$}
    % \organizationEng
    %     {$^1$Organization; $^2$Organization}
    % \abstractEng
    %     {This document is an example of paper prepared with \LaTeXe\
    %     typesetting system and style file \texttt{jmlda.sty}.
    
    %     \bigskip
    %     \textbf{Keywords}: \emph{keyword, keyword, more keywords}.}
        
    \begin{document}

    \maketitle
    \section{Введение}
        
        Для описания различных данных широко используются временные ряды.
        Чтобы найти их сходство вводится функция расстояния, однако стандартный поточечный подход не является информативным вследствие того,
        что ряды могут содержать общие паттерны, деформированные относительно временной оси: претерпевшие сдвиги либо сжатия \cite{01f4ab11a9ff49ff909094a135dcfe33}.
        Одним из способов решения этой проблемы является выравнивание временных рядов (DTW)  \cite{Keogh01derivativedynamic} и его модификаций \cite{journals/ida/SalvadorC07}.
        Этот подход в большом спектре задач позволяет достичь максимального качества среди его аналогов.
        
        В работе рассматривается применения DTW для кластеризации в случае многомерных временных рядов.
        Использование DTW на подобных данных описано в \cite{Holt2007}, \cite{Sanguansat2012MultipleMS}.
        В работе \cite{Holt2007} предлагается способ выравнивания многомерных рядов, основанный на нормализации исходных данных и нахождениии векторной нормы.
        В \cite{Sanguansat2012MultipleMS} рассматривается алгоритм, позволяющий выполнить выравнивание временных рядов между координатами. 
        Многомерное DTW предполагает различные варианты выравнивания, такие как выравнивание относительно общей временной шкалы и между соответсвующими каналами.
        
        В процессе работы алгоритма DTW происходит вычисление расстояний между точками сравниваемых рядов.
        Поскольку в многомерном случае координаты точек описываются векторами, на результат будет влиять выбор функций расстояния между ними.
        Исследование влияние выбора этих функций на качество кластеризации является главной особенностью этой работы.
        В работе используются функции расстояния порождённые $L_1$ и $L_2$ нормами.
        
        Ещё одним стандартным подходом к нахождению сходства между рядами является сравнение представления рядов коэффициентами их регрессионных моделей.
        Полученная в ходе работы DTW кластеризация сравнивается кластеризацией на основе авторегрессионной модели.
    
    % вставить обзор методов из статьи! 
        В статьях \cite{WARRENLIAO20051857} \cite{AGHABOZORGI201516} рассматриваются различные виды алгоритмов кластеризации временных рядов,
        среди которых неплохие результаты показывают варианты иерархической кластеризации.
        Данный вид кластеризации был выбран в качестве базового.
        
		% проапдейтить инфу о данных 
        Данные \cite{Kwapisz:2011:ARU:1964897.1964918} представляют собой измерения акселерометра некоторого носимого устройства,
        например мобильного телефона, находящегося в кармане человека, и используется для индентификации действия человека в конкретный момент времени.
        Данные разделены на 6 классов: ходьба, бег, подъём по лестнице, спуск по лестнице, сидение, лежание.
                
    \section{Постановка задачи}
		
				Временнным рядом называется упорядоченная последовательность $S_i = s_1,s_2,...,s_n$, где $n$ \-- длина временного ряда, $i\in l$, $l$ \-- количество каналов.
				
				Поскольку мы используем многомерные временные ряды, то $s_i$, $i \in n$ представляют собой вектор размерности m. Например, при рассмотрении задачи идентификации определенного движения человека вектор является трёхмерным с координатами (x, y, z).
        
        Пусть задано множество временных рядов $\mathbb{S} \subset \mathbb{R}^{l \times n}$.

        $\forall S_i \in \mathbb{S}$ задано ${y_i \in \mathbb{Y}}$ \-- множество меток классов.

        Пусть есть множество функций расстояния между векторами $\mathrm{R}$:
        $$
            \mathrm{R} = \{\rho: \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}^+ \}
        $$
				
				Пусть $S_i$, $S_j$ $\in \mathbb{S}$, тогда определим некоторые метрики, которые возмём в качестве функций расстояния между фекторами: 
				
				1. $L_2$ : $ED(S_i, S_j) = \sqrt{\displaystyle\sum_{k = 1}^{n} (s_i_k - s_j_k)^2}$
				% #todo добавить нормы 
				
				Функция расстояния рассчитывается для каждой координаты вектора $s_i$, $i \in n$ независимо от отстальных координат.
				
				Исходные данные представляют собой временные ряды (длиной n), однако в первом эксперименте мы будем использовать подпоследовательности врeменных рядов длины $m\in n$, чтобы выделить в исходном временном ряде определенные области, которые являются общеизвестными паттернами.

        Для каждой функции расстояния между векторами существует соответствующая функция расстояния между временными рядами:
        $$
            g_{\rho}: \mathbb{S} \times \mathbb{S} \rightarrow \mathbb{R}^+ 
        $$

        Возьмем выборку $ S \subset \mathbb{S}, \ \ |S| = N$

        Определим матрицу попарных расстояния:
        $$
            D(g_\rho(S)) = ||D_{ij}||, \ \ D_{ij} = g_\rho(S_i, S_j),\ \ S_i, S_j \in S 
        $$
        
        Определим кластеризатор:
        $$
            f: D \rightarrow Z^N
        $$

        Где $Z$ \-- множество меток кластеров.
				
				%пересмотреть метрики качества

        Будем рассматривать следующие функции качества:
        \begin{align*}
            Q_1(f(D), S) = \frac{1}{|Z|}\sum\limits_{z \in Z} \max_y \frac{N_z^y}{N_z}  \\
            Q_2(f(D), S) = \frac{1}{|Z|}\sum\limits_{z \in Z} \max_y \frac{(N_z^y)^2}{N_z N^y}
            % Q(f(D), S) = \frac{\sum\limits_{s_i, s_j \in S} \mathbb{I}(z_i = z_j \land  y_i = y_j)}{N^2}            
        \end{align*}
        Здесь: 
        \begin{itemize}[label=$\bullet$]
            \item $N_z$ \-- количество элементов в кластере с меткой $z$. 
            \item $N^y$ \-- количество элементов в классе $y$.
            \item $N_z^y$ \-- количество элементов класса $y$ в классе $z$.
        \end{itemize}

        Тогда, решаемая задача:
        $$
            Q_i(D(g_\rho(S), S) \rightarrow \max_{\rho}
        $$

        % Пусть $s_i \in \mathbb{R}^n $ - временные ряды, являющиеся объектами некоторой выборки,
        % $\rho$ - некоторые выбранные метрики, $Q$ - критерий качества (данные с соизмеримым количеством точек), являющийся ответом.
        % Требуется постороить такую функцию выравнивания, что:
        % $ f = argmin(Q(s_i, \rho))$
                
    \section{Описание основных методов}
        
        Для построения функции выравнивания и проверки её качества используются модель DTW (и её отптимазации).
            
    \paragraph{Описание функции расстояния между объектами}

        В данной работе в качестве метрического расстояния между объектами предлагается использовать строимость \textit{пути наименьшей стоимости} между объектами.
            
        Dynamic time warping - измерение расстояния между двумя временными рядами.
            
        Задано два временных ряда, X длины n и Y длины m.
            \begin{align*}
                X &= x_1,x_2, ..., x_i, ..., x_n \\
                Y &= y_1,y_2, ..., y_j, ..., y_m \\
                & x_i, y_j \in \mathbb{R}^n
            \end{align*}

        Требуется построить матрицу размера $n\times m$ c элементами $D_{ij}=d(x_i, y_j)$, где d - выбранная метрика.
            
        Чтобы найти наибольшее соответсвие между рядами нужно найти выравнивающий путь W, который минимизирует расстояние между ними.
        W - набор смежных элементов матрицы D, $w_k = (i, j)_k$.
            
            $W = w_1,w_2, ..., w_k, ..., w_K $

            $max(n, m)\leq K \leq m+n+1$, где K-длина выравнивающего пути
            
        Выравнивающий путь должен удовлетворять следующим условиям:
            \begin{enumerate}
                \item $w_1=(1,1)$, $w_K=(n, m)$
                \item $w_k = (a, b)$, $w_{k-1}=(a', b')$ : $a-a' \leq 1$, $b-b' \leq 1$ 
                \item $w_k = (a, b)$, $w_{k-1}=(a', b')$ : $a-a' \geq 0$, $b-b'\geq 0$
            \end{enumerate}

        Оптимальный выравнивающий путь должен минимизировать выравнивающую стоимость пути:
            $$
                DTW(X, Y)=\displaystyle\sum\limits_{k=1}^{K} w_k
            $$
            
        Путь находится рекуррентно:\\
            $\gamma(i, j) = d(q_i, c_j) + min({\gamma(i-1, j-1), \gamma(i-1, j), \gamma(i, j-1)})$ ,
            где $\gamma(i, j)$ суммарное расстояние, $d(q_i, c_j)$ расстояние в текущей клетке.
						
				Кроме того, выравнивающий путь ограничивают тем, насколько он может отклоняться от диагонали. Типичным ограничением является полоса Сако-Чиба, в которой говорится, что путь искривления не может отклоняться от диагонали больше, чем на определённый процент клеток.
						
				\paragraph{Оптимизации}
				
				\subparagraph{Использование квадрата расстояния}	
				В DTW и ED вычисляется квадратный корень, однако, если упустить этот шаг, относительное расстояние не изменится, поскольку обе функции монотонны и вогнуты. Это упрощает вычисления и позволяет сделать модель легкой для понимания. Таким образом, говоря о DTW и ED, мы подразумеваем их квадратные аналоги.
				
			  \subparagraph{Использование нижней границы}
				Для того чтобы ускорить последовательный поиск в DTW, используется нижняя граница (Lower Bounding), чтобы отбросить неподходящие последовательности (подпоследовательности). Оптимизация ускоряет поиск ещё и потому что не требует затратных вычислений.
				
				В эксперименте используется каскадная нижняя граница. Вначале последовательность проходит проверку на требования $LB_{Kim}$, которая использует расстояние между максимальными значениями рядов и минимальными значениями рядов. Однако, для того чтобы сравнить последовательности, они должны быть нормализованы, поэтому значения двух расстояний между максимальными и минимальными точками могут быть ничтожно малы. В случае если последовательность удовлетворила требованиям $LB_{Kim}$, происходит вторичная проверка $LB_{Keogh}$, использующей ED.
				
				Также бывает полезным менять роли сравниваемых последовательностей для $LB_{Keogh}$, от этого будет меняться решение к какой последовательности применяется нижняя граница, причем результаты вычисления этих границ для каждой из последовательностей при их сравнении в общем случае не будут равны. Однако данный метод применяется опционально и только в том случае, если другие нижние границы не проявили себя.
				
				\subparagraph{Использование верхней границы}
				При вычислении ED или $LB_{Keogh}$, мы можем заметить, что текущая сумма расстояний между каждой парой точек привысила определённое (наибольшее возможное) значение, в таком случае мы можем прекратить подсчёт, тк дальнейший действия дадут ещё более высокий результат.
				
				Если вся $LB_{Keogh}$ была посчитана и мы обнаружили, что должны вычислить DTW полностью, есть способ отбросить лишние вычислительные затраты на стадии подсчёта DTW. Если постепенно вычислять DTW слева направо от 1 до k и суммировать частичное накопление DTW с вкладом от $LB_{Keogh}$ от k+1 до n. Сумма $DTW(S_{1:k}^i, S_{1:k}^j) + LB_{Keogh}(S_{k+1:n}^i, S_{k+1:n}^j)$ является нижней границей для $DTW(S_{1:n}^i, S_{1:n}^j)$. Если в какой-то момент такая нижняя граница превысит верхнюю, расчёт прекращается. Кроме того, расходы на расчёт такой границы незначительны.
				
				Практически очевидно, что для сравнения двух временных рядов они должны быть нормализованы. Однако она занимает больше времени, чем подсчёт ED. Таким образом было решено объединить ED ($LB_Keogh$) c Z-нормализацией. Постепенно вычисляя нормализацию, мы в той же точке вычисляем ED ($LB_{Keogh}$). Это позволяет отбросить неподходящую последовательность не только на стадии подсчёта дистанции, но и на стадии нормализации.
				
				Во многих случаях становится полезным изменить порядок поиска верхней границы. Очевидно, что разный порядок поиска приносит разное ускорение, более того существует n! выриантов упорядочивания. Чтобы найти оптимальный вариант, предлагается отсортировать индексы основанных на абсолютных значениях Z-нормализованной последовательности. Одно значение временного ряда $S_i$ сравнивается со многими из ряда $S_j$, которые далее сортируются по убыванию вклада ED. Такая сортировка может быть применена как ED и $LB_{Keogh}$, так и к отбрасыванию неподходящей последовательности на стадии нормализации.
						
				\subparagraph{Использование многоядерных процессоров}
				Стоит отметить, что при правильном использовании можно добиться практически линейного ускорения с помощью многоядерных процессоров. Однако оптимизации методов полностью затмевают эти улучшения.
				
				\paragraph{План эксперимента}
						Эксперимент № 1
						
						1. глеб достал паттерны датасет соответствующий эпилепсии, склеил, добавил данные между ними
						
						2. Использовал алгоритм чтобы выделить паттерны среди данных из приступа эпилепсии прогнав многомерный массив с данными паттернов
						
						3. смотрим насколько успешно, кластеризуем и смотрим по метрике качетва
						
						4. Исследуем разные метрики дистанции (ищем лучшую)
						
						Эксперимент №2
						
						1. сравниваем пул рядов между собой и составляем матрицу их попарных расстояний чтобы кластеризовать на основе алгоритма из 1 эксперимента (оптимизации бесполезны не изем лучшую сравниваем одинаковую по длине, best so far просто может откидывать неподходящие )
						
						2. кластеризуем и смотрим метрикик дистанции и качества
						
						
      \iffalse
		    \paragraph{Описание алгоритма кластеризации}      
            В качестве алгоритма кластеризации используется иерархическая кластеризация, который базируется на последовательном слияние ближайших кластеров.
            Рассматриваются различные функции расстояния между кластерами: 
            \begin{enumerate}
                \item \textit{complete:}  $d(A, B) = \max\limits_{a \in A, b \in B}(dist(a, b))$ 
                \item \textit{weighted:}  $d(A,B) = \dfrac{(dist(S,B) + dist(T,B))}{2}$, где кластер $A = S \cup T$
                \item \textit{weighted:}  $d(u,v) = \sum\limits_{a \in A, b \in B} \dfrac{d(a, b)}{(|A|*|B|)}$ 
            \end{enumerate} 

        \paragraph{Описание авторегрессионного подхода}
            Авторегрессия - представляет собой подход, в котором элемент временного ряда представляется линейной комбинацией некоторого числа прошлых элементов.

            Пусть есть временной ряд $X = x_1, ... , x_n, \ x \in \mathbb{R}^n$.
            Размер окна авторегрессионной модели $l$.

            В модели авторегрессии $$ x_k = \sum\limits_{i=1}^{l} k_i \cdot x_{k-i}, \ k_i \in \mathbb{R} $$
            $\vec{k} = \{k_i\}$ \-- коэффициенты, обучаемые для наилучшего описания выборки.

            В дальнейшей работе с временным рядом, набор коэффициентов используется как вектор описания ряда.

            В нашей работе, на множестве векторов коэффициентов производится иерархическая классификация с $L_2$ расстоянием между векторами.

            % Также возможен вариант с независимыми коэффициентами для каждой координаты
            % $$ x_k = \sum\limits_{i=1}^{l} k_i \cdot x_{k-i}, \ k_i \in \mathbb{R} $$
  
			
    \section{Эксперимент}
        
        В ходе эксперимента были использованы данные акселерометра мобильного телефона.
        Они представляли собой временные ряды длинной в 600 точек ускорений по осям $X, Y, Z$.
        Из них была сгенерирована выборка из 2048 рядов по 50 точек.
        Каждых из рядов принадлежал к одному из шести возможных классов. Данные были равномерно распределены по всем классам.

        Проводилась кластеризация этих данных описаными методами. В качестве расстояния между векторами использовались $L_1$  и $L_2$ нормы.
        Так как в процессе получения данных были возможны различные положения телефона в кармане, кластеризация проводилась на 24, 36 и 48 кластеров.

        \paragraph{Результаты}
            \begin{center}
                \begin{tabular}{ll|rrr|rrr}  
                    \toprule
                    & & \multicolumn{3}{c|}{$Q_1$} & \multicolumn{3}{c}{$Q_2$} \\
                    \cmidrule(r){3-8}
                $\rho$ & n clust & \textit{complete} & \textit{average} & \textit{weighted} & \textit{complete} & \textit{average} & \textit{weighted} \\
                    \midrule
                $L_1$   & 24    &   0.5059  &   0.5854 &    0.6384  & 0.2732   &  0.3761    &   0.4488  \\
                        & 36    &   0.5325  &   0.6196 &    0.6163  & 0.2988   &  0.4246    &   0.4140  \\
                        & 48    &   0.5563  &   0.6388 &    0.6308  & 0.3303   &  0.4432    &   0.4306  \\
                $L_2$   & 24    &   0.4876  &   0.6216 &    0.6258  & 0.2701   &  0.4173    &   0.4246  \\
                        & 36    &   0.4982  &   0.6459 &    0.6433  & 0.2701   &  0.4545    &   0.4489  \\
                        & 48    &   0.5336  &   0.6486 &    0.6530  & 0.2701   &  0.4546    &   0.4615  \\
                \bottomrule
                \end{tabular}
            \end{center}
						
	    \fi

    \section{Заключение}

    \bibliography{literature} 
    \bibliographystyle{unsrt}
    
    % Решение Программного Комитета:
    %\ACCEPTNOTE
    %\AMENDNOTE
    %\REJECTNOTE
\end{document}
    

    
